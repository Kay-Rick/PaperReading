# 引言

## 数据中知识发现包括哪几个步骤

知识发现，即对数据进行选择和处理，自动发现新的、正确的、有用的模式，从而对现实世界中的对象进行建模

<div style="text-align: center"><img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121202015758.png" alt="image-20211121202015758" style="zoom: 50%;" /></div>

- **数据整理**：形成目标数据集
- **数据选择和预处理**：数据清理、数据归约和数据映射
- **数据挖掘**：选择算法并搜索感兴趣的模式或模型
- **解释和评估**：分析通过数据挖掘得到的结果



## 数据挖掘应用

数据分析、决策支持、对象识别、文本分类、文本挖掘、网络挖掘、流数据挖掘、语音识别、传感数据建模、无人驾驶、用户定制、疾病诊断、生物数据分析等。



# 学习的可行性

## NFL定理

首先看一个二分类的例子，输入特征x是二进制的、三维的。对应有8种输入，其中训练样本$D$有5个。根据训练样本对应的输出y，假设有8个$hypothesis$，这8个$hypothesis$在$D$上，对5个训练样本的分类效果效果都完全正确。但是在另外3个测试数据上，不同的$hypothesis$表现有好有坏。在已知数据$D$上$g\approx f$；但是在D以外的未知数据上$g\approx f$，不一定成立。

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121203804680.png" alt="image-20211121203804680" style="zoom: 67%;" />

而机器学习目的，恰恰是希望我们选择的模型能**在未知数据上的预测与真实结果是一致的**，而不是在已知的数据集$D$上寻求最佳效果。

这个例子告诉我们，我们想要在D以外的数据中更接近目标函数似乎是做不到的，只能保证对D有很好的分类结果。机器学习的这种特性被称为没有免费午餐（No Free Lunch）定理。

NFL定理表明没有一个学习算法可以在任何领域总是产生最准确的学习器。不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题，特定的先验信息，数据的分布，训练样本的数目，代价或奖励函数等。从这个例子来看，**NFL说明了无法保证一个机器学习算法在D以外的数据集上一定能分类或预测正确，除非加上一些假设条件**。

## Hoeffding不等式

在机器学习训练样本$D$的时候，如果样本$N$足够大，那么从样本中$h(x)\ne f(x)$的概率就可以推导在抽样样本以外的所有样本中$h(x)\ne f(x)$的概率。这里我们引入$E_{in}(h)$表示在抽样样本中$h(x)$和$f(x)$不相等的概率；$E_{out}(h)$表示实际所有样本中$h(x)$和$f(x)$不相等的概率

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121210519545.png" alt="image-20211121210519545" style="zoom:50%;" />
$$
P[|E_{in}(h) - E_{out}(h)| > \epsilon] \le2e^{(-2\epsilon^2N)}
$$
$Hoeffding$不等式说明当$N$很大的时候，抽样样本中不相等的概率$E_{in}(h)$和实际样本中不相等的概率$E_{out}(h)$，它们之间的差值被限定在$\epsilon$之内。

但是我们不能忽略训练样本D抽样出的那些Bad Data（抽样出的样本集并不能真正代表整体的情况），如果出现Bad Data的时候，就会出现$E_{in}(h)$和$E_{out}(h)$差别很大，但这只是小概率事件（例如：连续抛10次硬币均为正面朝上，这种情况会出现，但不能代表整体抛硬币的情况），我们依然计算在存在这种bad data情况下的概率

​													$P_D[BAD\ D]$

​													$= P_D[BAD\ D\ for\ h_1\ or\ BAD\ D\ for\ h_2\ or\ ...or\ BAD\ D\ for\ h_M]$

​													$\le P_D[BAD\ D\ for\ h_1] + P_D[BAD\ D\ for\ h_2] + ... + P_D[BAD\ D\ for\ h_M]$

​													$\le2exp(-2\epsilon^2N) + 2exp(-2\epsilon^2N) + ... + 2exp(-2\epsilon^2N)$

​													$= 2Mexp(-2\epsilon^2N)$

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121213949779.png" alt="image-20211121213949779" style="zoom:50%;" />

其中$M$是$hypothesis$的个数，$N$是$D$中样本的数量，这表明当$M$的个数有限，且$N$足够大的时候，Bad Data出现的概率就更低了，即能够保证$D$对于所有的$h$都有$E_{in}\approx E_{out}$，满足$PAC$，这时我们仍然可以像以前一样，选择使$E_{in}$最小的$h_M$作为$g$，至此就证明了这种情况下机器学习是可行的

## 说明学习可行性

$$
P[|v - \mu| > \epsilon] \leq 2e^{-2\epsilon^2N}
$$

- $v$表示样本中目标部分，该部分已知
- $\mu$表示总体数据中目标概率，该部分未知
- $N$表示样本规模
- $\epsilon$表示$v$和$\mu$之间的间隔

由不等式可以推得：当样本数$N$足够大，$v$和$\mu$就可以看作近似相等，即可以根据已知情况推出未知情形进行学习。此时学习误差$\approx$泛化误差，所以$N$很大的时候**学到的$h$用于泛化时的误差率接近学习误差率，保证低学习错误率就能得到好的泛化性能，所以学习是可行的**。

# 数据和数据预处理

## 数据属性

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121203141785.png" alt="image-20211121203141785" style="zoom: 67%;" />

## 非对称属性

**只有非零值才重要的二元属性是非对称的二元属性**

非对称属性中出现非零属性值才是重要的。考虑这样一个数据集，其中每一个对象是一个学生，而每个属性记录学生是否选修某个课程。对应某位学生，如果其选修了某属性的课程，则该属性取值为1，否则取值为0。**由于学生只选修可选课程中的很少一部分，这种数据集的大部分值为0，因此，关注非零值将更有意义**。



## 数据对象之间相似度、相异度计算

**相似度**

两个数据对象之间的相似性的数值度量。对象间越相似，相似度数值越大，并且数值常在范围$[0,1]$之间。0表示不相似，1表示完全相似

**相异度**

两个数据对象之间的不同性的数值度量。对象间越相似，相异度数值越小，并且数值常在$[0, \infty]$。通常相异度也称距离

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211122141017710.png" alt="image-20211122141017710" style="zoom: 67%;" />

### 闵可夫斯基距离

n维空间中两点坐标距离：
$$
d(\textbf{x},\textbf{y}) = \bigg (\sum _{k=1} ^{n} |x_k - y_k|^r \bigg )^{1/r}
$$

- $r = 1$：曼哈顿距离
- $r = 2$：欧几里得距离
- $r = \infty$：上确界距离，**当$r$趋于无穷大时闵可夫斯基距离可以转化为切比雪夫距离**$lim_{r\rightarrow \infty}\bigg (\sum _{k=1} ^{n} |x_k - y_k|^r \bigg )^{1/r} = max_{k=1}^{n}|x_k - y_k|$

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211122141916288.png" alt="image-20211122141916288" style="zoom:50%;" />



### 二元属性的相似性度量

**简单匹配系数（SMC）**
$$
SMC=\frac{值匹配的属性个数}{属性个数} = \frac{f_{11} + f_{00}}{f_{01} + f_{10} + f_{11} + f_{00}}
$$

- $f_{00}$：$x$取$0$并且$y$取$0$的属性个数
- $f_{01}$：$x$取$0$并且$y$取$1$的属性个数
- $f_{10}$：$x$取$1$并且$y$取$0$的属性个数
- $f_{11}$：$x$取$1$并且$y$取$1$的属性个数

**Jaccard系数**
$$
J=\frac{匹配的个数}{不涉及0-0匹配的属性个数} = \frac{f_{11}}{f_{01} + f_{10} + f_{11}}
$$
<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211122144207883.png" alt="image-20211122144207883" style="zoom:50%;" />

### 余弦相似度

通常，文档用向量表示，向量的每个属性代表一个特定的词(术语)在文档中出现的频率。尽管文档具有数以百千计或数以万计的属性，**但是每个文档向量都是稀疏的，因为它具有较少的非0属性，如果统计0-0匹配的情况，那么大多数文档都与其他大部分文档非常类似**。因此，文档的相似性度量不仅应该像Jaccard度量一样需要忽略0-0匹配，而且还必须能够处理非二元向量。余弦相似度就是常用度量之一。
$$
cos(\textbf{x}, \textbf{y}) = \frac{\textbf{x}·\textbf{y}}{||\textbf{x}||||\textbf{y}||}
$$
<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211122145559655.png" alt="image-20211122145559655" style="zoom:50%;" />



### 相关性(TODO)

### 马氏距离(TODO)



## 数据预处理的主要任务

数据预处理的主要任务:数据清洗、数据集成、数据变换、数据规约和数据离散化。

- **数据清洗**：例程通过填写缺失的值，光滑噪声数据，识别或删除离群点，并解决不一致性来清理数据
- **数据集成**：集成多个数据库、数据立方体或文件
- **数据变换**：数据规范化和聚集
- **数据规约**：数据集的简化表示，但产生同样的分析结果，包括维规约和数值规约
- **数据离散化**：数据规约的一部分，但是对于数值数据来说特别重要，通过将一个连续的属性划分为区间来减少数据体积



## 处理缺失值

**删除**

删除操作将存在遗漏信息属性值的样本行或者列删除，从而得到一个完整的数据表。删除操作简单易行，**适合缺失对象与初始数据集数量相比较小的情况下非常有效**。但当确实数据占比较大，这种处理缺失的方法就不合适。

**插补**

- **人工插补**：当我们对手头的数据集足够了解时，就可以自己填写缺失值。但是这种做法很费时
- **特殊值填充**：将空值作为一种特殊的属性值来处理。这种填充方式可能导致严重的数据偏离
- **平均值填充**：如果缺失的数据是数值型的，就根据该属性在其他所有对性的取值的平均值来填充该缺失的属性值。如果缺失的控制是非数值型的，就根据统计学中的众数原理，用该属性在其他所有对象的取值次数最多的值来补齐该缺失的属性值
- **就近补齐**：对于一个包含空值的对象，这种插补方法是在完整数据中找到一个与它最相似的对象，然后用这个给相似的对象的值来进行填充。这种插补加入的主观因素过多
- **K最近距离邻法**：先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的K个样本，将这K个值加权平均来估计该样本的缺失数据。这种插补法易于理解也易于实现，但是这种算法在分析大型数据集时会变得非常耗时
- **回归法**：基于完整的数据集，建立回归方程，或利用机器学习中的回归算法。对于包含空值的对象，将已知属性值代入回归方程来估计未知属性值，以此估计值来进行填充。这种做法常用
- **期望值最大化方法**：通过不完全数据情况下计算极大似然估计或者后验分布的迭代算法，不断进行迭代直至收敛

## 处理数据噪声(TODO)

# 决策树

## 基本思想

决策树由结点(node)和有向边(directed edge) 组成。结点有两种类型：**内部结点(internal node)和叶结点(leaf node)。内部结点表示一个特征或属性，叶结点表示一个类**。用决策树进行分类，从根节点开始，对实例分配到其子节点；这时，每一个子节点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至到达根节点，最后将实例分类到叶节点的类中

**决策树学习**

- 输入需要分类的数据集和类别标签
- **选择最佳属性**：评估每一个属性并选择最好的一个属性k作为决策树的根节点
- **划分数据集**：按照该属性k的每个取值划分数据集为若干部分
- **递归重复**：对于每个子节点，选择该子节点中最好的一个属性作为该子节点的值，然后按照上述划分数据集的方式划分该子节点。不断的重复此过程，直到满足一定的条件

## 错误率、熵和信息增益

在决策树分类中，究竟选择哪个特征更好？按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类。**选择最佳划分的度量通常是根据划分后子女结点的不纯性的程度**。有以下几种方式衡量不纯性：

**熵**
$$
Entropy(t) = -\sum _{i = 0} ^{c -  1} p(i|t)log_2 p(i|t)
$$
**熵越大，随机变量的不确定性就越大**

**基尼系数**
$$
Gini(t) = 1 - \sum _{i = 0} ^{c - 1}[p(i|t)]^2
$$

**错误率**
$$
Classfication\ Error(t) = 1 - max_i[p(i|t)]
$$
其中$p(i|t)$表示给定节点$t$中属于类$i$的记录所占的比例，注意：当我们使用分类错误率来进行分类的时候，属性的选择我们是选择错误率最小的那个属性进行分类。

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211122211612686.png" alt="image-20211122211612686" style="zoom:67%;" />



**信息增益**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211122211832839.png" alt="image-20211122211832839" style="zoom:50%;" />

从上面的例子我们可以看出，不同的不纯性度量是一致的，$N_1$有最低的不纯性度量值，接下来依次是$N_2,N_3$。

为了确定测试条件的效果，我们需要比较父节点（划分前）的不纯程度和子女节点（划分后）的不纯程度，它们的差越大，测试条件的效果就越好。增益$\Delta$就是可以用来确定划分效果的标准：
$$
\Delta = I(parent) - \sum _{j = 1} ^{k} \frac{N(v_j)}{N}I(v_j)
$$

- $I(·)$是给定节点的不纯性
- $N$是父节点的记录总数
- $k$是属性值的个数
- $N(v_j)$是与子女节点$v_j$相关联的记录个数

决策树算法通常选择最大化增益$\Delta$的测试条件，我们可以看到$I(parent)$是一个不变的值，所以最大化增益等价于最小化子女节点的不纯性度量的加权平均值。

如果我们**选用熵作为不纯性度量**的时候，这时候的增益就是信息增益$\Delta _{info} = entropy(p) - (\sum _i ^k \frac{n_i}{n}entropy(i))$

> 从图中我们可以看出熵和Gini指标在区间[0, 0.5]都是单调递增的，而在区间[0.5, 1]都是单调递减的，但仍然有可能信息增益和Gini指标支持不同的属性，因为尽管这些度量具有相似的范围和单调的行为，但它们各自的增益不一定以相同的方式运行



## 连续属性值划分

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125195322832.png" alt="image-20211125195322832" style="zoom:50%;" />

## 缺失值影响

- 影响怎样去计算不纯净度
- 影响如何将有缺失值的实例分配给子节点
- 影响怎么对有缺失值的测试实例进行分类

## 混淆矩阵

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211122221354196.png" alt="image-20211122221354196" style="zoom:50%;" />

- 准确率$Accuracy(T) = \frac{a + d}{a + b + c + d} = \frac{TP + TN}{TP + TN + FP + FN}$：分类正确的实例数占总数的比例，易受数据分布的影响
- 精度（查准率）$Precision(p) = \frac{a}{a + c} = \frac{TP}{TP + FP}$​：判定为类中的数据有多少是正确的（越大，错判少，漏判多）
- 召回率（查全率）$Recall(r) = \frac{a}{a + b} = \frac{TP}{TP+FN}$​：类中有多少数据被学习模型正确的判别出来（越大，漏判少，错判多）

一般来说呢，鱼与熊掌不可兼得。如果你的模型很贪婪，想要覆盖更多的sample，那么它就更有可能犯错。在这种情况下，你会有很高的recall，但是较低的precision。如果你的模型很保守，只对它很sure的sample作出预测，那么你的precision会很高，但是recall会相对低。F度量就是综合考虑精度和召回率两个指标进行调和均值处理

- $F-measure(F) = \frac{2}{\frac{1}{r} + \frac{1}{p}}=\frac{2rp}{r + p} = \frac{2a}{2a + b + c}$：综合考虑精度和召回率两个指标

## 代价敏感学习

代价矩阵对将一个类的记录分类到另一个类的惩罚进行编码。令$C(i,j)$表示预测一个$i$类记录为$j$类的代价。使用这种记号，$C(+,-)$是犯一个假负错误的代价，而$C(-,+)$是产生一个假警告的代价。代价矩阵中的一个负项表示对正确分类的奖励。给定一个$N$个记录的检验集，模型$M$的总代价是：
$$
C_t(M) = TP\times C(+,+) + FP\times C(-,+) + FN\times C(+,-) + TN\times C(-,-)
$$
<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126223846065.png" alt="image-20211126223846065" style="zoom: 50%;" />



## ROC曲线

- 模型真正率$TPR$：模型正确预测的正样本的比例(ReCall)

$$
TPR=\frac{TP}{TP + FN}
$$

- 模型假正率$FPR$：模型被预测为正类的负样本比例

$$
FPR=\frac{FP}{TN + FP}
$$

在ROC曲线中，真正率(TPR)沿y轴绘制，而假正率(FPR)沿x轴绘制。

- $(0, 0)$表示$TPR = 0, FPR = 0$，即$TP = 0, FP = 0$，也就是分类器将所有样本都分为了负样本
- $(1, 1)$表示$TPR = 1, FPR = 1$，即$FN = 0, TN = 0$，也就是分类器将所有样本都分为了正样本
- $(1, 0)$表示$TPR = 0, FPR = 1$，即$TP = 0, TN = 0$，也就是分类器将所有正负样本全部分错
- **$(0, 1)$表示$TPR = 1, FPR = 0$，即$FN = 0, FP = 0$，也就是分类器没有分错一个样本，每个样本都被正确分类，这也就是我们的理想模型**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126225707307.png" alt="image-20211126225707307" style="zoom: 67%;" />

一个好的分类模型应该尽可能靠近图的左上角，而一个随即猜测的模型应位于连接点$(0,0),(1,1)$的主对角线上。

- ROC曲线有助于比较不同分类器的相对性能。在上图中，当$FPR$小于$0.36$时，$M_1$要好于$M_2$，而$FPR$大于$0.36$时$M_2$较好。很显然两种分类器各有各的长处。
- ROC曲线下方的面积(AUC)提供了评价模型的平均性能的另一种方法。如果模型是完美的，则它在ROC曲线下方的面积等于1。**如果一个模型好于另一个，则它的ROC曲线下方面积较大**。

## 分类器比较(TODO)

p116



## 评估分类器性能

### 保持方法

将被标记的原始数据划分成两个不相交的集合，分别称为训练集和检验集。在训练集上归纳分类模型，在检验集上评估模型性能。划分比例通常根据分析专家的判断

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123132027012.png" alt="image-20211123132027012" style="zoom:50%;" />

- 数据集$D = \{(v_1, y_1),...,(v_n, y_n)\}$划分为训练集$D_t$和测试集$D_h=D \textbackslash D_t$，那么模型的准确度为

$$
acc_h = \frac{\sum_{(d_i,y_i)\in D_h} \delta(L(D_t, d_i), y_i)}{h}
$$

- $L(D_t, d_i)$：表示测试集样本$d_i$在学得的目标函数上的输出
- $\delta(i, j)$：$i = j$时为1，否则为0

**局限性**

- **没有充分利用数据**
- **模型高度依赖训练集和检验集的构成**：训练集越小，模型的方差越大，另一个方面如果训练集太大，较小的检验集估计的准确率不可靠
- 训练集和验证集是相关的



### k折交叉验证

将数据集随机划分为$k$个互斥的子数据集，一次选择其中一个子集作为测试集，其余$k-1$作为训练集，重复$k$次，**从而让每个子集都做过测试集**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123133746473.png" alt="image-20211123133746473" style="zoom:50%;" />

- **模型准确度**

$$
acc_{cv} = \frac{\sum_{(d_i,y_i)\in D} \delta(L(D\textbackslash D_i, d_i), y_i)}{n}
$$

### 留一法

留一法是一种特殊的$k$折交叉验证，其中$k = N$，其中$N$是数据集大小。也就是每个检验集只有一个记录。

**优点**：使用尽可能多的训练记录，此外，检验集之间是互斥的，并且有效地覆盖了整个数据集

**缺点**：整个过程重复$N$次，计算开销大，此外，因为每个检验集只有一个记录，性能估计度量的方差偏高



## 欠拟合和过拟合

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123141452057.png" alt="image-20211123141452057" style="zoom:50%;" />

对于训练模型而言，我们不仅要求它对训练数据集有很好的拟合（训练误差），同时也希望它可以**对未知数据集（测试集）有很好的拟合结果（泛化能力）**，所产生的测试误差被称为泛化误差。度量泛化能力的好坏，最直观的表现就是模型的**过拟合**（overfitting）和**欠拟合**（underfitting）。过拟合和欠拟合是用于描述模型在训练过程中的两种状态。

### 欠拟合

欠拟合是指模型不能在训练集上获得足够低的误差。也就是说模型复杂度低，模型在训练上表现很差，没法学习到数据背后的规律。

欠拟合基本上都会发生在训练刚开始的时候，经过不断训练之后欠拟合应该不怎么考虑了。但是如果真的还是存在的话，可以通过**增加网络复杂度**或者在模型中**增加特征**，这些都是很好解决欠拟合的方法。

### 过拟合

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123142245169.png" alt="image-20211123142245169" style="zoom:50%;" />

就是模型复杂度高于实际问题，**模型在训练集上表现很好，但在测试集上却表现很差**，没有理解数据背后的规律，**泛化能力差**

**为什么出现过拟合**

- **训练数据集样本单一，样本不足**：如果训练样本只有负样本，然后那生成的模型去预测正样本，这肯定预测不准。所以训练样本要尽可能的全面，覆盖所有的数据类型
- **训练数据中噪声干扰过大**：噪声指训练数据中的干扰数据。过多的干扰会导致记录了很多噪声特征，忽略了真实输入和输出之间的关系
- **模型过于复杂**：模型太复杂，已经能够“死记硬背”记下了训练数据的信息，但是遇到没有见过的数据的时候不能够变通，泛化能力太差。我们希望模型对不同的模型都有稳定的输出。模型太复杂是过拟合的重要因素。

**如何解决过拟合**

- **获取或使用更多的数据**：我们从数据源头获得更多的数据，只要有足够多的数据支撑，让模型看到看到尽可能多的例外情况，那么模型就会不断修正，从而避免出现overfitting的情况

- **丢弃部分特征**：通过丢弃一些不能帮助我们正确预测的特征，可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙解决（例如PCA算法），这样也可以解决overfitting的问题

- **运用正规化手段解决过拟合问题**：参数太多会导致我们模型复杂度上升，容易过拟合，也就是我们训练误差会很小。正则化机制就是通过引入额外新信息来解决机器学习过程中过拟合问题，这种额外信息通常是模型复杂性带来的惩罚度。正则化手段可以保证我们的模型保持简单

## 决策树过拟合解决方案

- 尽早地停止树地增长
- 允许树过拟合数据，然后对树进行**后修剪操作**

# 分类技术

## 基于规则的分类器

**两个重要性质**

- **互斥规则**：规则集R中不存在两条规则被同一条记录触发，两条规则是相互独立的，此性质**确保每条记录至多被R中的一条规则覆盖**
- **穷举规则**：对属性值的**任意组合，R中都存在一条规则加以覆盖**，此性质确保每一条记录都至少被R中的一条规则覆盖。

**优点**

- 有决策树一样的表达能力
- 易于解释
- 易于产生
- 能够快速分类新实例
- 性能与决策树相当

**待解决问题**

- **如果规则集不是互斥的，那么一条记录可能被多条规则覆盖**，这些规则的预测可能会相互冲突，通常使用有序规则（**对规则设置优先级**）和无序规则（**每条被触发规则的后件看作是对相应类的一次投票**）加以解决
- **如果规则集不是穷举的**，那么必须添加一个默认规则$r_d:()→y_d$ 来覆盖那些未被覆盖的记录。默认规则的前件为空，当所有其他规则失效时触发。$y_d$ 是默认类，通常被指定为没有被现存规则覆盖的训练记录的多数类

## 序列覆盖算法

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124204833247.png" alt="image-20211124204833247" style="zoom:50%;" />

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124210744379.png" alt="image-20211124210744379" style="zoom: 67%;" />

规则是基于一种评估方式贪婪地增长起来的，一次提取规则一个类，决定那个类应该首先被生成的规则取决于当前流行的类和对给定类别的记录进行错误划分的成本。



## 支持向量机(TODO)

# 神经网络

## 神经网络如何学习

### 神经元模型

神经元接受来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值与神经元的阈值进行比较，**然后通过“激活函数”处理以产生神经元的输出**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123150633964.png" alt="image-20211123150633964" style="zoom:50%;" />

- $x_i$表示输出信号的值
- $w_i$对应的是权重，特别地我们注意$w_0$是偏置

### 激活函数

理想中的激活函数的是阶跃函数，它将输入值映射为输出值0或1，然而阶跃函数具有不连续、不光滑等不太好的性质，因此实际常用$Sigmoid(x) = \frac{1}{1 + e^{-x}}$函数作为激活函数，它把可能在较大范围内变化的输入值挤压到$(0,1)$输出值范围内

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123151728467.png" alt="image-20211123151728467" style="zoom: 67%;" />

除此以外，常用的激活函数还有符号函数、线性函数，ReLU等



## 梯度下降算法

### Batch Gradient Descent

**定义误差**
$$
E(\textbf w) = \frac{1}{2} \sum _{d\in D}(t_d - o_d)^2
$$

- $t_d$表示期望输出
- $o_d$表示实际输出
- $D$表示训练样本，**这里把所有样本的误差加起来这就是批处理(batch learning)**

**误差偏导**
$$
\Delta E(\textbf w) = \bigg [ \frac{\partial E}{\partial w_0},\frac{\partial E}{\partial w_1},...,\frac{\partial E}{\partial w_n} \bigg]
$$


**计算权值更新量**
$$
\Delta w_i = -\eta \frac{\partial E}{\partial w_i}
$$

$$
w_i \leftarrow w_i + \Delta w_i
$$

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123170100734.png" alt="image-20211123170100734" style="zoom:33%;" />

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123165949598.png" alt="image-20211123165949598" style="zoom:33%;" />

- 对于其中某一个权值$w_i$而言，我们的更新量**是沿梯度的反方向去更新**，因为我们知道沿函数梯度方向是函数值上升最快的方向，因此反方向是函数下降最快的方向，我们计算误差，当然是使误差更小的方向去做出改变，因此是梯度反方向
- $\eta$表示学习率：决定了每次改变$w_i$的幅度大小，并且我们可以发现每步的大小和导数成比例，在最小值附近斜率会越来越平缓，每步会越来越小

<img src="C:\Users\Rick\AppData\Roaming\Typora\typora-user-images\image-20211123164838908.png" alt="image-20211123164838908" style="zoom: 25%;" />

- **如果学习率太小，梯度下降会比较慢；反之学习率太大，可能超过最小值，导致最后无法收敛，甚至发散**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123165552266.png" alt="image-20211123165552266" style="zoom:50%;" />

- 对误差函数求偏导的过程中，我们假设实际输出$o(x)=w·x$是线性的，得到$w_i$的调整方式$\Delta w_i = \eta \sum _{d\in D}(t_d - o_d)x_{id}$，也就是$w_i$的调整方式(增大\减小)取决于期望输出与实际输出的差和当前输入$x_{id}$的正负

  

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123171616610.png" alt="image-20211123171616610" style="zoom:50%;" />

- 上图是BGD的算法逻辑，我们看到最大特点就是：**在每一次更新权重之前，利用全部训练样例的误差计算权重的更新值**，每一步都需要大量的计算



### Stochastic Gradient Descent

不同于批量梯度下降，随机梯度下降是在**每次迭代时**使用**一个样本**来对参数进行更新



## 神经网络学习优缺点

**优点**

- **拥有非线性映射能力**：尤其适用于内部机制复杂的问题
- **拥有自学习和自适应能力**：BP神经网络在训练时，能够通过学习自动提取输出、输出数据间的“合理规则”
- **黑盒模型**：不需要对产生的结果进行解释而且能够发现输入数据中不显式的新特性

**缺点**

- **学习过程收敛速度慢**：由于BP神经网络算法本质上是梯度下降法，它所要优化的目标函数非常负责，这势必会导致算法收敛速度慢的现象
- **局部最小化问题**：网络的权值通过沿局部改善的方向进行调整的，这样会使算法陷入局部极值，权值收敛到局部极小点，从而使网络训练失败
- **样本依赖性问题**：BP神经网络模型的逼近和推广能力与学习样本的典型性密切相关，而从问题中选取典型样本实例组成训练集是一个很困难的问题。
- **得到的网络性能差**
- **学习率不稳定**





## 提高神经网络泛化能力(TODO)



## 多层神经网络(TODO)

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123154206490.png" alt="image-20211123154206490" style="zoom: 50%;" />





# 贝叶斯学习

## 相关公式

### 全概率公式

设事件$B_1,B_2,...B_n$构成一个完备事件组，即它们两两不相容，和为全集且$P(B_i) > 0$，则对任一事件$A$有：
$$
P(A) = \sum_{i = 1}^{n}P(B_i)P(A|B_i)
$$


### 贝叶斯公式

$$
P(B_i|A) = \frac{P(B_i)P(A|B_i)}{P(A)} = \frac{P(B_i)P(A|B_i)}{\sum_{i = 1}^{n} P(B_i)P(A|B_i)}
$$

可以看出，贝叶斯公式是“由果溯因”的思想，当知道某件事的结果后，由结果推断这件事是由各个原因导致的概率为多少

### 先验概率

指根据以往经验和分析。在实验或采样前就可以得到的概率，在贝叶斯公式中就是$P(B_i)$

### 后验概率

指某件事已经发生，根据观察到的样本想要计算这件事发生的原因是由某个因素引起的概率，在贝叶斯公式中就是$P(B_i|A)$

### 条件独立

给定随机变量的集合$Z,X,Y$，$X$和$Y$相互独立
$$
P(X|Y,Z) = P(X|Z)
$$

### 推导

$P(W|R)=P(W|R,S)P(S|R) + P(W|R,\neg S)P(\neg S|R)$

$P(W|C)=P(W|R,S)P(R,S|C) + P(W|\neg R,S)P(\neg R,S|C) + P(W|R,\neg S)P(R,\neg S|C)  + P(W|\neg R,\neg S)P(\neg R,\neg S|C) $



## 频率学派&极大似然估计

频率学派认为世界是确定的。他们直接为事件本身建模，也就是说事件在多次重复实验中趋于一个稳定的值p，那么这个值就是该事件的概率。他们认为模型参数是个定值，希望通过类似解方程组的方式从数据中求得该未知数。这就s是频率学派使用的参数估计方法-**极大似然估计（MLE）**

假设数据 $x_1,x_2,...x_n$ 是独立同分布的一组抽样，$X=(x_1,x_2,...x_n)$ 那么MLE对$\theta$的估计方法可以如下推导

- $p(x|\theta)$中的 $\theta$ 是一个常量
- 对于 $N$ 个观测来说观测集的概率为

$$
p(X|\theta)\mathop{=}\limits _{iid}\prod\limits _{i=1}^{N}p(x_{i}|\theta))
$$



- 极大对数似然：

$$
\theta_{MLE}=\mathop{argmax}\limits _{\theta}\log p(X|\theta)\mathop{=}\limits _{iid}\mathop{argmax}\limits _{\theta}\sum\limits _{i=1}^{N}\log p(x_{i}|\theta)
$$

通俗理解来说，**就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124142915850.png" alt="image-20211124142915850" style="zoom:50%;" />

## 贝叶斯学派&极大后验概率估计

他们认为世界是不确定的，因获取的信息不同而异。假设对世界先有一个预先的估计，然后通过获取的信息来不断调整之前的预估计。 他们认为模型参数源自某种潜在分布，希望从数据中推知该分布。对于数据的观测方式不同或者假设不同，那么推知的该参数也会因此而存在差异这就是贝叶斯派视角下用来估计参数的常用方法-**最大后验概率估计（MAP）**

- $p(x|\theta)$ 中的 $\theta$ 不是一个常量。这个 $\theta$ 满足一个预设的先验的分布 $\theta\sim p(\theta)$ 。于是根据贝叶斯定理依赖观测集参数的后验可以写成：

$$
p(\theta|X)=\frac{p(X|\theta)\cdot p(\theta)}{p(X)}=\frac{p(X|\theta)\cdot p(\theta)}{\int\limits _{\theta}p(X|\theta)\cdot p(\theta)d\theta}
$$

- 为了求 $\theta$ 的值，我们要最大化这个参数后验MAP（其中第二个等号是由于分母和 $\theta$ 没有关系）


$$
\theta_{MAP}=\mathop{argmax}\limits _{\theta}\ p(\theta|X)=\mathop{argmax}\limits _{\theta}\ p(X|\theta)\cdot p(\theta)
$$

## 贝叶斯定理分类

$X$表示属性集，$Y$表示类变量。如果类变量和属性之间的关系不确定，那么我们可以把$X$和$Y$看作随机变量，用$P(Y|X)$以概率的方式捕捉二者之间的关系。这个条件概率又称为$Y$的后验概率(posterior probability)，与之相对地，$P(Y)$称为$Y$的先验概率(prior probability)
$$
P(Y|\textbf X) = \frac{P(\textbf X | Y)}{P(\textbf X)}
$$

- 在比较不同$Y$值的后验概率时，**分母$P(\textbf X)$总是常数，因此可以忽略**
- 先验概率$P(Y)$可以通过计算训练集中属于每个类的训练记录所占的比例很容易地估计
- 对于类条件概率$P(\textbf X | Y)$的估计，可以使用朴**素贝叶斯分类器和贝叶斯信念网络**





## 最小描述长度(MDL)

**最小长度描述准则**
$$
h_{MDL} = argmin_{h\in H}L_{C_1}(h) + L_{C_2}(D|h)
$$

- 最优编码：已知消息$i$出现的概率是$p_i$，最优编码对消息$i$的编码长度为$-log_2p_i$
- $L_C(x)$是在编码方案$C$下对$x$进行编码的最小位数
  - $L_{C_1}(h)$是假设$h$的比特位上描述长度
  - $L_{C_2}(D|h)$是数据$D$在假设$h$下编码时的比特位上描述长度
- $L(h)$大表示模型较为复杂，说明对数据$D$有很好的拟合，因此$L(D|h)$小
- $L(h)$小表示模型较为简单，说明对数据$D$拟合程度程度不够，因此$L(D|h)$大
- 我们可以看出：**当他们的和最小时，假设既简单，且也比较合适**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125234552215.png" alt="image-20211125234552215" style="zoom: 67%;" />

最小描述长度的目的是**根据信息论中的基本概念来解释极大后验假设(MAP)**
$$
h_{MAP} = argmax_{h\in H} P(h|D) = argmax_{h\in H}\frac{P(D|h)P(h)}{P(D)} = argmax_{h\in H}P(D|h)P(h)
$$
对上述极大后验假设取对数

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124153207834.png" alt="image-20211124153207834" style="zoom:50%;" />



> MDL原理就用于贝叶斯网络的学习。该度量被视为网络结构的描述长度和在给定结构下样本数据集的描述长度之和。一方面，用于描述网络结构的编码位**随模型复杂度的增加而增加** ； 另一方面， 对**数据集描述的编码位随模型复杂度的增加而下降**。因此，贝叶斯网络的 MDL总是**力求在模型精度和模型复杂度之间找到平衡**



## 贝叶斯最优分类器

**基本思想**

新的实例的最可能的分类是通过**所有假设的预测的后验概率加权求和得到**
$$
argmax_{v_j\in V} \sum_{h_i \in H}P(v_j|h_i)P(h_i|D)
$$

- $V$使分类可以取的所有值的集合
- $v_j$是一种可能的分类
- $P(h_i|D)$表示假设的可能性
- $P(v_j|h_i)$表示在假设$h_i$下分类结果是$v_j$的概率

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123233916159.png" alt="image-20211123233916159" style="zoom:50%;" />



## 朴素贝叶斯分类器

朴素贝叶斯分类器在估计类条件概率时假设属性之间条件独立
$$
P(\textbf X | Y=y) = \prod _{i = 1}^{d}P(X_i|Y=y)
$$
其中每个属性集$X=\{X_1,X_2,...X_d\}$包含$d$个属性，有了条件独立的假设，就不必计算$X$的每一个组合的类条件概率，只需要给定$Y$，计算每一个$X_i$的条件概率
$$
P(Y|\textbf X) = \frac{P(Y)\prod_{i=1}^{d} P(\textbf X_i | Y)}{P(\textbf X)}
$$
**由于对所有$Y,P(X)$是固定的，因此只要找出使分子$P(Y)\prod_{i=1}^{d} P(\textbf X_i | Y)$最大的类就足够**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123231539958.png" alt="image-20211123231539958" style="zoom:50%;" />

### 分类属性条件概率

![image-20211123222302129](https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123222302129.png)

### 连续属性条件概率

#### 离散化估计

可以把一个连续的属性离散化，然后用相应的离散区间替换连续属性值。这种方法把连续属性转换成叙述属性。通过计算类$y$的训练记录中落入$X_i$对应的区间的比例来估计条件概率$P(X_i|Y=y)$

#### 随机变量服从高斯分布

这种方法可以假设连续变量服从某种概率分布，然后使用训练数据估计分布的参数，高斯分布通常被用来表示连续属性的类条件概率分布。该分布有两个参数，均值$\mu$和方差$\sigma^2$。对于每个类$y_j$，属性$X_i$的类条件概率等于：
$$
P(X_i = x_i|Y = y_j) = \frac{1}{\sqrt{2\pi}\sigma_{ij}}e^{-\frac{(x_i - \mu_{ij})^2}{2\sigma_{ij}^2}}
$$

- 参数$\mu_{ij}$可以用类$y_j$的所有训练记录关于$X_i$的样本均值来估计
- 参数$\sigma_{ij}^2$可以用这些训练记录的样本方差$s^2$来估计

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211123224752730.png" alt="image-20211123224752730" style="zoom: 67%;" />



## 贝叶斯信念网络

背景：朴素贝叶斯分类器的条件独立假设过于严格，特别是对那些属性之间有一定相关性的分类问题。我们引入贝叶斯信念网络模型进行推理。

### 模型表示

贝叶斯信念网络(BBN)简称贝叶斯网络，用**图形表示一组随机变量之间的概率关系**

- 一个有向无环，表示变量之间的依赖关系
- **一个概率表把各结点和它的直接父结点关联起来**

### 性质

> 贝叶斯网络中的一个结点，如果它的父母结点已知，则它条件独立于它的所有非后代结点

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125232708464.png" alt="image-20211125232708464" style="zoom:67%;" />

如图b所示：给定C，则A条件独立与B和D，因为B和D都是A的非后代结点。朴素贝叶斯分类其中条件独立假设也可以用贝叶斯网络表示。如图c所示，其中y是目标类，$\{X_1,X_2,...,X_d\}$是属性集。

除了网络拓扑架构要求的独立性以外，每个结点还关联一个概率表：

- 如果结点$X$没有父母结点，则表中只包含先验概率$P(X)$
- 如果结点$X$只有一个父母结点$Y$，则表中包含条件概率$P(X|Y)$
- 如果结点X有多个父母结点$\{Y_1,Y_2,...Y_k\}$，则表中包含条件概率$P(X|Y_1,Y_2,...,Y_k)$

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125233456903.png" alt="image-20211125233456903" style="zoom:50%;" />

一般解题步骤：进行拼凑，然后利用相互独立结点之间的独立性性质展开运算

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125234143807.png" alt="image-20211125234143807" style="zoom:50%;" />

## 偏差方差分析(TODO)



# 基于实例学习

## K近邻算法

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124160632003.png" alt="image-20211124160632003" style="zoom:50%;" />

K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例**最邻近**的K个实例，**这K个实例的多数属于某个类**，就把该输入实例分类到这个类中。

### 算法原理

- 假设有一个**带有标签的样本数据集（训练样本集）**，其中包含每条数据与所属分类的对应关系
- 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较
  - 计算新数据与样本数据集中每条数据的距离（一般我们使用欧式距离）
  - **对求得的所有距离进行排序**（从小到大，越小表示越相似）
  - 取前k（k 一般小于等于 20 ）个样本数据对应的分类标签
  - 求k个数据中**出现次数最多**的分类标签作为新数据的分类

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124161318550.png" alt="image-20211124161318550" style="zoom:67%;" />



### k值选择

**k值过小**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124161542626.png" alt="image-20211124161542626" style="zoom:50%;" />

当k值过小，对红色的额五边形进行分类时，比如我们取极端情况k=1，我们就很容易将将五边形分类成黑色原点。这其实就使模型过于复杂，很容易学习到噪声，也非常容易判定为噪声类别，产生了过拟合现象。

**评价**：较小的k值能够更好地捕捉空间的结构，需要小训练集，并且计算开销较小，但是对噪声较为敏感

**k值过大**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124162124267.png" alt="image-20211124162124267" style="zoom:50%;" />

我们继续以对极端的情况，k=N（N为训练样本个数），那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时候的模型就过于简单

**评价**：对噪声点不敏感，对离散的类估计很好，需要较大的数据集

**实践方法**：我们一般选取一个较小的数值，通常采取交叉验证法来选取最优的k值。



### 距离归一化

首先举例如下，我用一个人身高(cm)与脚码（尺码）大小来作为特征值，类别为男性或者女性。我们现在如果有5个训练样本，分布如下：

A [(179,42),男] B [(178,43),男] C [(165,36)女] D [(177,42),男] E [(160,35),女]

很容易看到第一维身高特征是第二维脚码特征的4倍左右，那么在进行距离度量的时候，**我们就会偏向于第一维特征。**这样造成俩个特征并不是等价重要的，最终可能会导致距离计算错误，从而导致预测错误

因此距离归一化的目的是：**防止某一维度的数据的数值大小对距离计算产生影响。多个维度的特征是等权重的，所以不能被数值大小影响。防止距离计算失效**

一种常用的归一化方式是：
$$
x_{nor} = \frac{x_t - \overline{x_t}}{\sigma_t}
$$

- $x_t$是第$t$维属性的均值
- $\sigma_t$表示第$t$维属性的方差

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124164329329.png" alt="image-20211124164329329" style="zoom: 50%;" />



### 适用场景

- 实例的属性少于20维
- 有大量的训练数据

### 算法优点

- 训练快
- 学习复杂目标函数
- 不会丢失信息

### 算法缺点

- 分类慢
- 容易受无关属性影响



## 局部加权线性回归

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124190730581.png" alt="image-20211124190730581" style="zoom:50%;" />

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124190740524.png" alt="image-20211124190740524" style="zoom:50%;" />

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124190749535.png" alt="image-20211124190749535" style="zoom:50%;" />

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124190758573.png" alt="image-20211124190758573" style="zoom:50%;" />

## 基于案例的学习与k近邻学习的异同

**相同点**

- 它们都是**懒惰学习**，直到分类实例来的时候才开始学习
- 在对新实例进行分类时，**他们都是对与新实例相似的实例进行分析，忽略与新实例不同的实例**

**不同点**

- KNN用实际的点表示实例，基于案例的推理使用**更加丰富的符号表示实例**
- 基于实例的推理获取相似实例的方法**更加复杂**，并且能够应用于概念更加复杂的问题

## 懒惰学习

当查询到来时才进行推理（k 近邻，加权线性回归）。创造局部估计。**训练时间短，查询时间长**。对于同一个假设空间H，可以表示更复杂的函数，同时**提供多个不同局部近似假设**

## 积极学习

**查询到来之前进行推理**（径向基函数网络，决策树，反向传播，朴素贝叶斯），创造一个全局估计，**训练时间长，查询时间短**。对于同一个假设空间H，**积极学习可以提供唯一一个全局近似假设**

# 集成学习

## 定义

集成学习是一种集中多个模型进行预测的技术，以便预测所得的结果比使用一个模型得到的结果更好。集成学习组合多个假设可以得到一个更好的假设，即集合多个弱学习器得到一个强学习器。



## 集成学习的两个主要问题

- **如何产生基学习器**：同构-异构；操作训练集（Bagging；Boosting；DECORATE）；操作属性集（集成属性选择）；操作输出（错误纠正输出码）；操作算法（随机注射）
- **怎样集成基学习器**：（加权）投票；（加权）取平均值；学习组合器：Stacking（一般组合器），RegionBoost（分段组合器）



## Bagging

**基本思想**

- 使用bootstrap来产生L个训练样本（**有放回抽样构造数据集**）
- 使用不稳定的学习过程训练L个基学习器：不稳定的学习过程使训练集变化很小，基学习器变化很大(决策树，多层感知器)
- **通过多数投票集成基学习器**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211117180956893.png" alt="image-20211117180956893" style="zoom:50%;" />

**伪代码**

- 通过$boostrap$得到$L$个训练样本
- 得到$L$个分类器$\in \{-1,1\}:c_1,c_2 ,c_3 ,...,c_L$或者$L$ 个评估概率$\in[0,1]：p_1, p_2 , p_3 ,..., p_L$
- 聚合分类器$c_{bag}(x)=sign(\frac{1}{L} \sum _{b=1}^{L} c^b(x))$或$p_{bag}(x) = \frac{1}{L} \sum _{b=1} ^{L} p^b(x)$



## Boosting

**基本思想**

- 重新加权实例代替bagging中的抽样，开始时，给每一个训练实例一个**相同的权重**
- 在每次迭代中，学习一个新的假设（弱学习者），**并对实例进行重新加权，以使系统集中在最近学习的分类器错误的实例中**
- 最终分类基于弱分类器加权投票

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211117201832989.png" alt="image-20211117201832989" style="zoom:50%;" />

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211117201850190.png" alt="image-20211117201850190" style="zoom:50%;" />

**伪代码**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124191711869.png" alt="image-20211124191711869" style="zoom: 50%;" />



## 集成学习为何有效

集成学习的一个充分必要条件是：如果基分类器准确且多样，那么这些分类器的集成将比其任何单个成员更准确。

- **从统计方面来看**：由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到相同的性能，此时若使用单学习器，可能因误选而导致泛化能力不佳，**结合多个学习器则会减少这一风险**
- **从计算方面来看**：在有足够训练数据的情况下，学习算法在计算上可能仍然很难找到最佳目标函数，许多学习算法往往会陷入局部最小，有的局部极小点所对应的泛化性能可能很糟糕，**而集成学习通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险**，这样就有可能找到一个全局最优的假设
- **从表示的方面来看**：在机器学习的大多数应用中，真实函数目标函数f不能由任何假设表示；**通过形成从H得出的假设的加权和，可以扩展可表示函数的空间**。某些学习任务的真实假设可能不在当前学习算法所考虑的假设空间中，此时若使用单学习器则肯定无效，而通过结合多个学习器，**由于相应的假设空间有所扩大，有可能学得更好的近似**

# 聚类分析

## 聚类定义

聚类，是一个把数据对象（或观测）划分成子集的过程。每个子集是一个簇，使得簇中的对象彼此相似，但与其他簇中的对象互不相同。由聚类分析产生的簇的集合称作一个聚类。

## 聚类类型

- 层次聚类（嵌套）：以层次树的形式组合在一起的嵌套簇的集合。
- 划分聚类（非嵌套）：各个簇之间不重叠，每一个数据对象只属于一个簇中。
- 排斥聚类：非排斥的聚类中，点可以属于多个簇；排斥聚类可以代表多个类或边界点。
- 重叠聚类：一个点可以属于多个簇，簇与簇之间相互有重叠。
- 模糊聚类：模糊聚类中，一个点属于每一个簇的权重在（0,1）之间，所有权重之和为1。概率聚类具有相似的特征。
- 完全聚类：完全聚类将每一对象都划分给一个簇。
- 部分聚类：没有将每一对象都划分给一个簇。

## 簇的类型

- **分离很好的簇**：簇内距离小于簇间距离

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124213401514.png" alt="image-20211124213401514" style="zoom: 33%;" />

- **基于中心点的簇**：一个簇中的对象离该簇中心点比离其他簇的中心点的距离近。中心点：通常是质心（簇中所有点的均值）或中（簇中最具代表性的点：簇中最中间的对象）

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124213435147.png" alt="image-20211124213435147" style="zoom: 33%;" />

- **基于邻近性的簇**：基于图的簇的一个特例，一个簇是一组相互连接的对象，这些象与该簇外的对象不相连。簇中的每一个点到该簇中的一个点或多个点的距离要比到其他簇的任意点的距离近。当簇不规则或有交织时有用，但有噪声的时候不可用

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124213517688.png" alt="image-20211124213517688" style="zoom:33%;" />

- **基于密度的簇**：簇是一个密集的点区域，由低密度区域与高密度的其他区域分开。当簇不规则、交织或有噪声时有用

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124213544061.png" alt="image-20211124213544061" style="zoom:33%;" />

- **共享属性（基于概念）的簇**：簇是一组共享一些公共属性或表示特定的概念的对象。此簇的概念可以概括之前类型的簇

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124213613667.png" alt="image-20211124213613667" style="zoom:33%;" />

- **目标函数定义的簇**：寻找最大化或最小化目标函数的簇。此簇可以有全局（划分聚类）或局部（层次聚类）的目标



## 定义簇间相似性

### Single Link(MIN)

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124221952531.png" alt="image-20211124221952531" style="zoom:50%;" />

簇之间的邻近度定义为两个不同簇中两个点间的最小距离（最大相似性）。易于处理不同大小规模的簇；对噪声点和异常点敏感。

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124224153529.png" alt="image-20211124224153529" style="zoom:67%;" />

### Complete Link(MAX)

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124222530426.png" alt="image-20211124222530426" style="zoom:50%;" />

簇之间的邻近度定义为两个不同簇中两个点间的最大距离（最大相似性），每次仍将两个最近的簇合并；对噪声点和异常点的影响不太敏感；可能会分割较大的簇

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124224209274.png" alt="image-20211124224209274" style="zoom:67%;" />

### Group Average

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124222625544.png" alt="image-20211124222625544" style="zoom:50%;" />

簇间距离为两个簇间所有两点距离的平均值。

### Distance Between Centroids

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124222715995.png" alt="image-20211124222715995" style="zoom:50%;" />

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124224234741.png" alt="image-20211124224234741" style="zoom:67%;" />

## 层次聚类

> 以一种层次树的结构来组织簇。事先不需要知道簇的数目，可以在合适的水平进行分割。

### 凝聚层次聚类

计算邻接矩阵，采用**自底向上**策略。刚开始时**每一个点作为一个簇，每次选两个最近的簇合并**直到只剩下一个（或k 个）簇。

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124221045979.png" alt="image-20211124221045979" style="zoom:50%;" />

**Single link凝聚层次聚类**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124225822357.png" alt="image-20211124225822357" style="zoom: 67%;" />

**Complete Link凝聚层次聚类**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124230005024.png" alt="image-20211124230005024" style="zoom: 67%;" />

### 分裂层次聚类

采用自顶向下策略，刚开始时所有点为一个簇，每次选一个簇划分为两个簇直到每个簇只包含一个点（或剩下k 个簇）

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211124220955206.png" alt="image-20211124220955206" style="zoom:50%;" />

## K-Means

K-Means是发现给定数据集的K个簇的聚类算法, 之所以称之为K-均值是因为**它可以发现$K$个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成**

![image-20211124224458513](C:\Users\Rick\AppData\Roaming\Typora\typora-user-images\image-20211124224458513.png)

### 指派点到最近的质心

为了将点指派到最近的质心，我们需要邻近性度量来量化所考虑的数据的“最近”概念。通常，随欧氏空间中的点使用欧几里得距离（$L_2$）

### 质心和目标函数

考虑到邻近性度量为欧几里得距离，我们使用**误差的平方和（Sum of the Squared Error, SSE）**作为度量聚类质量的目标函数，也就是我们计算每个数据点的误差，即它到之心的欧几里得距离，然后计算误差的平方和。给定由多次运行k均值产生的两个不同的簇集，我们更喜欢误差的平方和最小的那个。因此整体的误差平方和为：
$$
SSE = \sum_{i = 1}^{K}\sum_{x\in C_i}dist(c_i,x)^2
$$
**使簇的$SSE$最小的质心是均值。第$i$个簇的质心**：
$$
c_i = \frac{1}{m_i}\sum_{x\in C_i}\textbf x
$$


- $dist$是欧几里得空间中两个对象之间的标准欧几里得距离($L_2$)
- $\textbf x$表示对象
- $C_i$表示第$i$个簇
- $m_i$表示第$i$个簇中对象的个数
- $K$表示簇的个数

![image-20211125145303523](https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125145303523.png)

k均值算法的步骤三和步骤四试图直接最小化SSE，然而K均值步骤三和步骤四只能**确保找到关于SSE的局部最优**，因为它们是对选定的质心的簇，而不是对所有可能的选择来优化SSE

### 选择初始质心

常见选取质心是**随机确定的**（不一定是数据中的点），但是簇的质量常常很差。

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125151115569.png" alt="image-20211125151115569" style="zoom:50%;" />

因此常常使用其他技术进行初始化。一种有效的方法是：**去一个样本并使用层次聚类技术对它进行聚类，从层次聚类中提取K个簇。并用这些簇的质心作为初始质心**。该方法通常在**样本相对较小，K对于样本大小较小的情况下比较有效**。

### 附加问题

**预处理**

数据规范化；删除异常点。

**后处理**

删除可能代表异常点的小簇；分割松动的簇（SSE 值相对较大）；合并离得近的簇（SSE 值相对较小）

**处理空簇**

如果所有点在指派步骤都未分配到某个簇，就会得到空簇，如果这种情况发生，则需要某种策略来选择一个替补质心。

- **选择一个距离当前任何质心最远的点**，这将消除当前对总平方误差影响最大的点
- 在具有最大SSE的簇中选择一个替补质心，这种分裂簇并降低聚类的总SSE，如果有多个空簇，则过程重复多次

**离群点(TODO)**

### 评价

K均值算法简单并且可以用于各种数据类型。它也相当有效。但是K均值算法会得到**局部最优解，需要事先确定k值**；并且**不能处理噪声、异常点；并且不适合发现非凸形状的簇**。



## 二分K均值（TODO）



## K中心点算法(TODO)

不再使用一个簇中对象的平均值作为参考点，使用**最中点位置的对象作为簇中心**（PAM）

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125155438413.png" alt="image-20211125155438413" style="zoom:50%;" />

## DBScan

DBScan(Density Based Spatial Clustering of Applications with Noise)是具有噪声应用的**基于密度的空间聚类**，其将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，用于在具有噪声的空间数据库中**发现任意形状的聚类**。

首先对点进行分类：

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125161845281.png" alt="image-20211125161845281" style="zoom:50%;" />

- **$Eps$**：邻域最大半径
- **$MinPts$**：该点的EPS邻域中的最小点数
- **$Density$**：指定半径内的点数
- **核心点**：邻居数大于$MinPts$（该点的$EPS$邻域中的最小点数）
- **边界点**：属于核心点的邻居，但不是核心点
- **噪声点**：既不是核心点也不是边界点

### 密度可达

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125162411835.png" alt="image-20211125162411835" style="zoom: 50%;" />

### 算法步骤

- 首选**任意选取一个点**，然后找到到这个点距离小于等于eps的所有的点。如果距起始点的距离在eps之内的数据点个数小于 MinPts，那么这个点被标记为**噪声。**如果距离在 eps 之内的数据点个数大于MinPts，则这个点被标记为**核心样本**，并被分配一个新的簇标签。

- 然后访问该点的所有邻居（在距离 eps 以内）。如果它们还没有被分配一个簇，那么就将刚刚创建的新的簇标签分配给它们。如果它们是核心样本，那么就依次访问其邻居，以此类推。簇逐渐增大，直到在簇的eps距离内没有更多的核心样本为止。

- **选取另一个尚未被访问过的点，并重复相同的过程**

### 参数选取



### 评价

**优点**

- 不需要用户先验地设置簇的个数，可以划分具有复杂形状的簇
- 聚类速度快且能够有效处理噪声点
- **可以发现任意形状的空间聚类**

**缺点**

- 当空间聚类的密度不均匀、聚类间距差相差很大时，聚类质量较差

## 聚类评估

### 监督评估

对于监督类型，将聚类分析的结果与外界已知的结果进行比较，监督度量通常称为**外部指标**

### 非监督评估

对于非监督类型的簇评估，也就是不考虑外部信息。例如SSE

**凝聚度和分离度**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125182851312.png" alt="image-20211125182851312" style="zoom:50%;" />

**轮廓系数**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211125183903834.png" alt="image-20211125183903834" style="zoom: 67%;" />





### 相对指数(TODO)

用于比较两个不同的 聚类方法或簇 。 **通常一个外部或内部指标被用于这个函数**，例如， 平方误差和或熵

# 关联分析



## 关联分析相关概念

![image-20211126143810194](https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126143810194.png)

### 项集&支持度计数

令$I=\{i_1,i_2,...i_d\}$是购物揽数据中所有项的集合，而$T=\{t_1,t_2,...t_N\}$是所有事务的集合。每个事务$t_i$包含的出现而项集都是$I$的子集。如果一个项集包含$k$个项，则称它为$k-$项集。

项集的一个重要性质是它的**支持度计数**
$$
\sigma(X) = |\{t_i|X\subseteq t_i, t_i \in T\}|
$$
其中$|·|$表示集合中元素的个数，例如某个项集$X=\{啤酒,尿布,牛奶\}$的支持度计数为为2，表示有2个事务同时包含这三个项

### 支持度&置信度

我们知道关联规则是**形如$X\rightarrow Y$的蕴含表达式**，其中$X$和$Y$是不相交的项集，即$X\cap Y = \varnothing$。

支持度$(s)$可以用于**给定数据集的频繁程度**，某一个项集占总交易条目的比例；项集同时包含$X,Y$的概率
$$
s(X\rightarrow Y) = \frac{\sigma(X\cup Y)}{N}
$$
置信度$(c)$可以**确定$Y$在包含$X$的事务中出现的频繁程度**，测量$X$中包含$Y$的事务中出现的项目的频率，即包含$X$的交易同时包含$Y$的条件概率
$$
c(X\rightarrow Y) = \frac{\sigma(X\cup Y)}{\sigma(X)}
$$
<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126144638944.png" alt="image-20211126144638944" style="zoom:67%;" />

### 频繁项集

给定事务的集合T，关联规则发现是指找出**支持度大于等于$minsup$并且置信度大于等于$minconf$的所有规则**，其中$minsup$和$minconf$是对应的支持度和在置信度阈值。**频繁项集**就是支持度大于或等于**最小支持度阈值**的项集

包含$d$项的数据集可以提取的可能规则总数为$R = 3^d-2^{d+1} + 1$，这种计算开销呈指数级别增长，然而在实际中，有大部分的计算都是无用的开销，为了避免进行不必要的计算，事先对规则进行剪枝，而无需计算它们的支持度和置信度。

### 极大频繁项集

任意**直接超集都是非频繁的频繁项集**是极大频繁项集，极大频繁项集有效地提供了**频繁项集的紧凑表示**。换句话说，极大频繁项集形成了可以导出所有频繁项集的最小的项集的集合。但是找到极大频繁项集不能提供它们子集的支持度的任何信息。

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211119153600639.png" alt="image-20211119153600639" style="zoom:33%;" />

### 闭频繁项集

如果一个项集的直接超集都不具有和它相同的支持度计数，项集X被称为是**闭项集**。闭项集提供了频繁项集的一种最小表示，**该表示不丢失支持度信息**。

如果一个项集是**闭项集并且它的支持度大于或等于最小支持度阈值**，那么这个项集被称为闭频繁项集。

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211119160859929.png" alt="image-20211119160859929" style="zoom: 50%;" />

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211119160526060.png" alt="image-20211119160526060" style="zoom:50%;" />

## Apriori算法

**背景**

比如我们要找到支持度大于0.8的所有项集，通常的一种做法就是通过枚举所有可能的组合，然后对每一种组合统计它出现的频繁程度。然而这种做法会随物品的增加呈现出指数级别的增长。**因此Apriori算法本质是进行一种剪枝操作**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211119135039908.png" alt="image-20211119135039908" style="zoom:33%;" />

**原则**

- **Apriori定律1**：如果一个集合是频繁项集，则它的所有**子集**都是频繁项集

  <img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211119134958473.png" alt="image-20211119134958473" style="zoom: 33%;" />

- **Apriori定律2**：如果一个集合不是频繁项集，则它的所有**超集**都不是频繁项集

  <img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211119135145759.png" alt="image-20211119135145759" style="zoom:33%;" />



**伪代码**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211119141120554.png" alt="image-20211119141120554" style="zoom: 50%;" />

- $C_k$为候选$k-$项集的集合，$L_k$为频繁$k-$项集的集合
- **该算法初始通过单遍扫描数据集合，确定每个项的支持度**。一旦完成这一步，就可以得到所有频繁$1-$项集合$L_1$
- 重复以下操作：
  - $candidate(L_k)$：从长度为$k$的频繁项集中产生长度为$k+1$的候选频繁项集
  - $counting$：扫描数据库，对每一个候选频繁项集进行计数
  - $filtering$：删除非频繁项集，留下频繁项集
- 最终将过滤得到的所有频繁项集组合返回

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126155503744.png" alt="image-20211126155503744" style="zoom:50%;" />

## Hash树(TODO)



## FP增长算法

FP增长是一种自底向上方式搜索树，由FP树产生频繁项集的算法。

### 构建FP树

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126160813480.png" alt="image-20211126160813480" style="zoom:67%;" />

FP树是一种输入数据的压缩表示，它通过逐个读取事务，并把每个事务映射到FP树中的一条路径来构造。由于不同的事务可能会有若干个相同的项，因此他们的路径可能部分重叠。路径相互重叠越多，使用FP树结构获得的压缩效果越好。

### 提取后缀结点路径

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126161545489.png" alt="image-20211126161545489" style="zoom:67%;" />

算法通过处理后缀节点相关联的路径去寻找相关频繁项集，直到处理完所有结点，假设最小支持度为2得到的频繁项集如下：

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126162244448.png" alt="image-20211126162244448" style="zoom:67%;" />

FP增长采用分治策略将一个问题分解为较小的子问题，从而发现某个特定后缀结尾的所有频繁项集。

例如我们对e结尾的的频繁项集感兴趣。为了实现这个目的

- 检查项集e本身是否频繁。如果它是频繁的，则考虑发现以de结尾的频繁项集子问题。接下来是ce和ae，依次，每一个子问题又可以进一步划分为更小的子问题，然后合并结果
- **根据抽取出的以e结尾的前缀路径去生成e的条件FP树**
  - **更新前缀路径上的支持度计数**，因为前缀路径上可能存在不包含项e的事务（例如b，c）
  - 删除e结点，修改前缀路径，因为沿这些前缀路径的支持度计数已经更新，以反映包含e的那些事务
  - 更新前缀路径上的支持度计数后，某些项可能不再是频繁的，例如结点b只出现了1次，它的支持度计数为1，这就意味着只有一个事务同时包含b和e，所以可以忽略b
- 使用e的条件FP树来解决发现以de，ce和ae结尾的频繁项集的子问题

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126172654009.png" alt="image-20211126172654009" style="zoom:67%;" />

我们可以看到FP增长算法中，每一次递归都要通过更新前缀路径中的支持度计数和删除非频繁项来构建条件FP树。



## 关联模式评估

关联规则算法往往产生太多的规则：很多都是无趣的或冗余（如果{A, B, C}->{D}和 {A、 B}->{D}有同样的支持度和置信度就是冗余的）在关联规则的原始公式中，支持度和置信度是唯一使用的度量 。

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126195155320.png" alt="image-20211126195155320" style="zoom:50%;" />

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211126195215819.png" alt="image-20211126195215819" style="zoom:50%;" />

# 维度约减

## 为什么要进行降维？

- 降低时间复杂度：减少计算
- 降低空间复杂度：更少的参数
- 节省观察特征的成本
- 更简单的模型在小数据集上更稳健
- 更具可解释性； 更简单的解释

## 维度约减方法

### 特征选择

定义：直接从原属性选择出一些重要属性，删除不重要属性。

- **过滤式**选择（无监督）：**先对数据集进行特征选择，然后再训练学习器**。通常可选用相关系数、互信息、信息增益等对选取的特征子集进行评价。**特征选择过程与后续学习器无关**，这相当于先对初始特征进行“过滤”，再用过滤后的特征训练模型。
  - 优点：**执行速度快、泛化能力强**
  - 缺点：**可能会选择大的子集**。由于目标函数通常是单调的，所以可能选择全集作为最优集

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121153931271.png" alt="image-20211121153931271" style="zoom:50%;" />

- **包裹式**选择（有监督）：**从初始特征集合中不断的选择特征子集，训练学习器，根据学习器的性能来对子集进行评价，直到选择出最佳的子集**。
  - 优点：分类器和数据集进行了确切的交互，所以能实现**高识别率**；在预测准确率上**使用交叉验证法，能够避免过拟合**
  - 缺点：执行速度慢，为每一个属性子集训练一个分类器（如果使用交叉验证的话可能要多个分类器），在计算复杂的问题中这种方法不可行

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121155334912.png" alt="image-20211121155334912" style="zoom:50%;" />

### 特征提取

定义：新属性由原属性映射得到

- 线性映射：主成分分析（PCA）、线性判别分析（LDA）、因子分析（FA）、多维标度、典型相关分析
- 非线性映射：等距特征映射、局部线性嵌入、拉普拉斯特征图



## 特征搜索

给定一个特征集$X=\{x_i|i=1,2,…,N\}$，寻找一个优化目标函数$J(X’)$(也可以正确分类) 的子集$X’_M=\{x_{i1},x_{i2},…,x_{iM}\}(M<N)$

### 为什么进行特征搜索？

- 删除冗余属性
- 删除不相关属性
- 考虑交互属性（一个属性本身可能和目标概念没有什么关系，但是当它和目标概念结合的时候就与目标概念有很大联系）

### 五种特征搜索方法

#### 朴素顺序属性选择

**基本思想**：单独评估每个属性，根据每个属性的预测效果对属性进行排序（从高到低），选出前面得分最高的N个属性组成特征子集

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121165839080.png" alt="image-20211121165839080" style="zoom:50%;" />

**评价**：没有考虑属性之间的依赖性，因而性能不太好

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121163633532.png" alt="image-20211121163633532" style="zoom:50%;" />

#### 顺序前向选择(SFS)

**基本思想**：初始化最优属性子集Y为空集；**选一个识别率最高的属性加入Y**；选一个与已选属性组合识别率最高的属性加入Y；重复选择属性加入Y直到识别准确率**不再提高**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121170211232.png" alt="image-20211121170211232" style="zoom:50%;" />

**评价**：只能加入特征不能去除特征，贪心算法，容易陷入局部最优值

#### 顺序后向选择(SBS)

**基本思想**：初始化最优属性子集Y为全集。从Y中**移除一个识别率最低的属性**。从Y中选一个属性移除后使剩下属性识别率最高的属性移除。重复选择属性移除直到**识别准确率开始下降**

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121170815859.png" alt="image-20211121170815859" style="zoom:50%;" />

**评价**：只能去除特征不能加入特征，贪心算法，容易陷入局部最优值

#### 双向搜索(BDS)

**基本思想**：双向搜索是SFS和SBS的并行实现。SFS从空集开始，SBS从全集开始。其中SFS已选择属性不会被SBS删除SBS；已删除属性不会被SFS添加

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121171225610.png" alt="image-20211121171225610" style="zoom:50%;" />

**评价**：BDS结合了SFS与SBS，**其时间复杂度比SFS与SBS小，但是兼有SFS与SBS的缺点**

#### 顺序浮动前向选择(SFFS)

**基本思想**：初始最优属性子集Y为空集，每一步使用SFS选择一个最好属性加到Y中，然后使用SBS从Y中选择最差属性移除直到识别率开始降低；重复SFS和SBS上述选择操作直到SFS无法进行

<img src="https://kay-rick.oss-cn-beijing.aliyuncs.com/img/image-20211121172114421.png" alt="image-20211121172114421" style="zoom:50%;" />

## 维度约减评估

对维度约减效果的评估，通常是比较降维前后学习器的性能（时间复杂度和空间复杂度），若性能有所提高则认为降维起到了作用，若将维数降至二维或三维，则可通过可视化技术来直观地判断降维效果。
